{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = 'D:/Astro/StarCatalogues'\n",
    "file_name = 'Imm Deep Sky Compendium - 2024 - rev5a.xlsm'\n",
    "file_path = os.path.join(file_dir, file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(file_path, sheet_name=\"Main\", skiprows=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix up the column headers\n",
    "\n",
    "# Extract column names for columns 0 to 33, and from row 0 for columns 34 onward\n",
    "header = list(data.columns[:34]) + data.iloc[0, 34:].tolist()\n",
    "substitutions = {\n",
    "    'Object Name & Image': 'Object',\n",
    "    'Unnamed: 1': 'Simbad',\n",
    "    'Unnamed: 2': 'Aladin',\n",
    "    'Right Ascension': 'RA_hms',\n",
    "    'Unnamed: 12': 'RA_deg',\n",
    "    'Declination': 'Dec_dms',\n",
    "    'Unnamed: 14': 'Dec_deg',\n",
    "    'Const.': 'Constellation',\n",
    "    'Nick.': 'Name',\n",
    "    'Alt. ID': 'Name_Alt',\n",
    "    'Surf.': 'Brightness',\n",
    "    'Inclin.': 'Inclination',\n",
    "    17: 'Next17',\n",
    "    19: 'Next19',\n",
    "    21: 'Next21',\n",
    "    23: 'Next23',\n",
    "    'Unnamed': 'Rename',\n",
    "}\n",
    "header[-6]='ObjAz'\n",
    "header[-5]='Key'\n",
    "header[-4]='FOV'\n",
    "header = [substitutions.get(col, col) for col in header]\n",
    "index = [col_idx for col_idx, col_name in enumerate(header) if col_name == 'Abell']\n",
    "header[index[0]]='AbellG'   # Replace duplate Abell with AbellG\n",
    "header[index[1]]='AbellN'   # Replace duplate Abell with AbellG\n",
    "data.columns = header\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop junk columns and first 2 rows (header data)\n",
    "columns_to_drop = ['Simbad', 'Aladin', 'My', 'My.1', 'My.2', 'Next17', 'Next19', 'Next21', 'Next23', 'Time', 'Date', 'Alt.', 'Separ.', 'Index', 'ObjAz', 'Transit Time', 'Add. Filters', 'Sort to ']\n",
    "data = data.drop(columns=columns_to_drop)\n",
    "data = data.iloc[2:].reset_index(drop=True)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['RA_deg'] = pd.to_numeric(data['RA_deg'], errors='coerce')\n",
    "data['Dec_deg'] = pd.to_numeric(data['Dec_deg'], errors='coerce')\n",
    "data['Size'] = pd.to_numeric(data['Size'], errors='coerce')\n",
    "data['Visual'] = pd.to_numeric(data['Visual'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['NGC'] = pd.to_numeric(data['NGC'], errors='coerce').astype('Int64')\n",
    "data['IC'] = pd.to_numeric(data['IC'], errors='coerce').astype('Int64')\n",
    "data['H400'] = pd.to_numeric(data['H400'], errors='coerce').astype('Int64')\n",
    "data['Messier'] = pd.to_numeric(data['Messier'], errors='coerce').astype('Int64')\n",
    "data['Caldwell'] = pd.to_numeric(data['Caldwell'], errors='coerce').astype('Int64')\n",
    "data['Arp'] = pd.to_numeric(data['Arp'], errors='coerce').astype('Int64')\n",
    "data['Hickson'] = pd.to_numeric(data['Hickson'], errors='coerce').astype('Int64')\n",
    "data['AbellG'] = pd.to_numeric(data['AbellG'], errors='coerce').astype('Int64')\n",
    "data['AbellN'] = pd.to_numeric(data['AbellN'], errors='coerce').astype('Int64')\n",
    "data['UGC'] = pd.to_numeric(data['UGC'], errors='coerce').astype('Int64')\n",
    "data['PGC'] = pd.to_numeric(data['PGC'], errors='coerce').astype('Int64')\n",
    "data['Griff.'] = pd.to_numeric(data['Griff.'], errors='coerce').astype('Int64')\n",
    "data['Kohou.'] = pd.to_numeric(data['Kohou.'], errors='coerce').astype('Int64')\n",
    "data['Mink.'] = pd.to_numeric(data['Mink.'], errors='coerce').astype('Int64')\n",
    "data['Barn.'] = pd.to_numeric(data['Barn.'], errors='coerce').round().astype('Int64') # has decimals\n",
    "data['Gum'] = pd.to_numeric(data['Gum'], errors='coerce').astype('Int64')\n",
    "data['LBN'] = pd.to_numeric(data['LBN'], errors='coerce').astype('Int64')\n",
    "data['LDN'] = pd.to_numeric(data['LDN'], errors='coerce').astype('Int64')\n",
    "data['Sh2'] = pd.to_numeric(data['Sh2'], errors='coerce').astype('Int64')\n",
    "#data['SNR'] = pd.to_numeric(data['SNR'], errors='coerce').astype('Int64')  # has text\n",
    "data['vdB'] = pd.to_numeric(data['vdB'], errors='coerce').astype('Int64')\n",
    "data['HT'] = pd.to_numeric(data['HT'], errors='coerce').astype('Int64')\n",
    "data['SD'] = pd.to_numeric(data['SD'], errors='coerce').astype('Int64')\n",
    "data['OB'] = pd.to_numeric(data['OB'], errors='coerce').astype('Int64')\n",
    "data['SP'] = pd.to_numeric(data['SP'], errors='coerce').astype('Int64')\n",
    "data['FG'] = pd.to_numeric(data['FG'], errors='coerce').astype('Int64')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the type column\n",
    "data['Type'] = data['Type'].replace({'Neb ':'Neb'})\n",
    "type_abbr = {\n",
    "    'Neb': 'Nebula',\n",
    "    'Gal': 'Galaxy',\n",
    "    'Stars': 'Cluster',\n",
    "    'Star': 'Star',\n",
    "}\n",
    "type_enum = {abbr: idx for idx, abbr in enumerate(type_abbr.keys())}\n",
    "C1_lookup = {idx: name for idx, name in enumerate(type_abbr.values())}\n",
    "data['C1'] = data['Type'].map(type_enum)\n",
    "data['C1Name'] = data['C1'].map(C1_lookup)\n",
    "\n",
    "unique_abbr = set(data['Type'].unique())\n",
    "valid_abbr = set(type_abbr.keys())\n",
    "# Find any values not in the lookup\n",
    "invalid_abbr = unique_abbr - valid_abbr\n",
    "missing_abbr = valid_abbr - unique_abbr\n",
    "\n",
    "if invalid_abbr:\n",
    "    print(f\"Invalid or unmapped abbreviations found: {sorted(invalid_abbr)}\")\n",
    "elif missing_abbr:\n",
    "    print(f\"Missing abbreviations found: {sorted(missing_abbr)}\")\n",
    "else:\n",
    "    print(\"All abbreviations are valid and mapped.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the constellation column\n",
    "data['Constellation'] = data['Constellation'].replace({'Apu':'Aps', 'Lmi': 'LMi', 'Uma': 'UMa', 'Crux': 'Cru', 'Cent': 'Cen', 'Pis': 'Pic'})\n",
    "constellation_abbr = {\n",
    "    'And': 'Andromeda', 'Ant': 'Antlia', 'Aps': 'Apus', 'Aql': 'Aquila', 'Aqr': 'Aquarius',\n",
    "    'Ara': 'Ara', 'Ari': 'Aries', 'Aur': 'Auriga', 'Boo': 'Boötes', 'CMa': 'Canis Major',\n",
    "    'CMi': 'Canis Minor', 'CVn': 'Canes Venatici', 'Cam': 'Camelopardalis', 'Cap': 'Capricornus',\n",
    "    'Car': 'Carina', 'Cas': 'Cassiopeia', 'Cen': 'Centaurus', 'Cep': 'Cepheus', 'Cet': 'Cetus',\n",
    "    'Cha': 'Chamaeleon', 'Cir': 'Circinus', 'Cnc': 'Cancer', 'Col': 'Columba', 'Com': 'Coma Berenices',\n",
    "    'CrA': 'Corona Australis', 'CrB': 'Corona Borealis', 'Crt': 'Crater', 'Cru': 'Crux',\n",
    "    'Crv': 'Corvus', 'Cyg': 'Cygnus', 'Del': 'Delphinus', 'Dor': 'Dorado', 'Dra': 'Draco',\n",
    "    'Eri': 'Eridanus', 'For': 'Fornax', 'Gem': 'Gemini', 'Gru': 'Grus', 'Her': 'Hercules',\n",
    "    'Hor': 'Horologium', 'Hya': 'Hydra', 'LMi': 'Leo Minor', 'Lac': 'Lacerta', 'Leo': 'Leo',\n",
    "    'Lep': 'Lepus', 'Lib': 'Libra', 'Lup': 'Lupus', 'Lyn': 'Lynx', 'Lyr': 'Lyra',\n",
    "    'Men': 'Mensa', 'Mic': 'Microscopium', 'Mon': 'Monoceros', 'Mus': 'Musca', 'Nor': 'Norma',\n",
    "    'Oct': 'Octans', 'Oph': 'Ophiuchus', 'Ori': 'Orion', 'Pav': 'Pavo', 'Peg': 'Pegasus',\n",
    "    'Per': 'Perseus', 'Pic': 'Pictor', 'PsA': 'Piscis Austrinus', 'Psc': 'Pisces', 'Pup': 'Puppis',\n",
    "    'Pyx': 'Pyxis', 'Ret': 'Reticulum', 'Scl': 'Sculptor', 'Sco': 'Scorpius', 'Sct': 'Scutum',\n",
    "    'Ser': 'Serpens', 'Sex': 'Sextans', 'Sge': 'Sagitta', 'Sgr': 'Sagittarius', 'Tau': 'Taurus',\n",
    "    'Tel': 'Telescopium', 'TrA': 'Triangulum Australe', 'Tri': 'Triangulum', 'Tuc': 'Tucana',\n",
    "    'UMa': 'Ursa Major', 'UMi': 'Ursa Minor', 'Vel': 'Vela', 'Vir': 'Virgo', 'Vol': 'Volans',\n",
    "    'Vul': 'Vulpecula'\n",
    "}\n",
    "const_enum = {abbr: idx for idx, abbr in enumerate(constellation_abbr.keys())}\n",
    "Cn_lookup = {idx: name for idx, name in enumerate(constellation_abbr.values())}\n",
    "data['Cn'] = data['Constellation'].map(const_enum)\n",
    "data['CnName'] = data['Cn'].map(Cn_lookup)\n",
    "\n",
    "\n",
    "\n",
    "unique_abbr = set(data['Constellation'].unique())\n",
    "valid_abbr = set(constellation_abbr.keys())\n",
    "# Find any values not in the lookup\n",
    "invalid_abbr = unique_abbr - valid_abbr\n",
    "missing_abbr = valid_abbr - unique_abbr\n",
    "\n",
    "if invalid_abbr:\n",
    "    print(f\"Invalid or unmapped abbreviations found: {sorted(invalid_abbr)}\")\n",
    "elif missing_abbr:\n",
    "    print(f\"Missing abbreviations found: {sorted(missing_abbr)}\")\n",
    "else:\n",
    "    print(\"All abbreviations are valid and mapped.\")\n",
    "data['Constellation'].unique()\n",
    "\n",
    "data['ConstName'] = data['Constellation'].map(constellation_abbr)\n",
    "\n",
    "grouped = data.groupby(['Constellation', 'Type'], observed=True).size().reset_index(name='Count')\n",
    "fig = px.bar(grouped, x='Constellation', y='Count', color='Type', title='Object Constellation by Type', \n",
    "             category_orders={'Constellation': list(constellation_abbr.keys())} ) \n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the type column\n",
    "data['Sub'] = data['Sub'].replace({'Group ':'Group', 'Pair ': 'Pair', 'Spiral ': 'Spiral', 'Mol CLd':'Mol Cld', 'Stars':'Star' })\n",
    "sub_abbr = {\n",
    "    # Multiple Galaxies\n",
    "    'Chain': 'Set of Chained Galaxies',\n",
    "    'Cluster': 'Set of Clustered Galaxies',\n",
    "    'Group': 'Set of Grouped Galaxies',\n",
    "    'Merger': 'Set of Merging Galaxies',\n",
    "    'Pair': 'Pair of Galaxies',\n",
    "    'Trio': 'Trio of Galaxies',\n",
    "\n",
    "    # Individual Galaxy\n",
    "    'BCD': 'Blue Compact Dwarf Galaxy',\n",
    "    'Coll': 'Collisional Ring Galaxy',\n",
    "    'Dwarf': 'Dwarf Galaxy',\n",
    "    'Ellip': 'Elliptical Galaxy',\n",
    "    'Floc': 'Flocculent Galaxy',\n",
    "    'Lent': 'Lenticular Galaxy',\n",
    "    'Mag': 'Magellanic Galaxy',\n",
    "    'Polar': 'Polar Galaxy',\n",
    "    'Spiral': 'Spiral Galaxy',\n",
    "\n",
    "    # Nebula\n",
    "    'Dark': 'Dark Nebula',\n",
    "    'Em': 'Emission Nebula',\n",
    "    'Mol Cld': 'Molecular Cloud Nebula',\n",
    "    'PN': 'Planetary Nebula',\n",
    "    'PPN': 'Protoplanetary Nebula',\n",
    "    'Refl': 'Reflection Nebula',\n",
    "    'SNR': 'Supernova Remnant Nebula',\n",
    "\n",
    "    # Stellar associations\n",
    "    'GC': 'Globular Cluster',\n",
    "    'HH': 'Herbig-Haro Object',\n",
    "    'Nova': 'Nova Object',\n",
    "    'OC': 'Open Cluster',\n",
    "    'Star': 'Star',\n",
    "    'Star Cld': 'Star Cloud',\n",
    "    'YSO': 'Young Stellar Object'\n",
    "}\n",
    "sub_enum = {abbr: idx for idx, abbr in enumerate(sub_abbr.keys())}\n",
    "C2_lookup = {idx: name for idx, name in enumerate(sub_abbr.values())}\n",
    "data['C2'] = data['Sub'].map(sub_enum)\n",
    "data['C2Name'] = data['C2'].map(C2_lookup)\n",
    "\n",
    "unique_abbr = set(data['Sub'].unique())\n",
    "valid_abbr = set(sub_abbr.keys())\n",
    "# Find any values not in the lookup\n",
    "invalid_abbr = unique_abbr - valid_abbr\n",
    "missing_abbr = valid_abbr - unique_abbr\n",
    "\n",
    "if invalid_abbr:\n",
    "    print(f\"Invalid or unmapped abbreviations found: {sorted(invalid_abbr)}\")\n",
    "elif missing_abbr:\n",
    "    print(f\"Missing abbreviations found: {sorted(missing_abbr)}\")\n",
    "else:\n",
    "    print(\"All abbreviations are valid and mapped.\")\n",
    "\n",
    "\n",
    "data['SubName'] = data['Sub'].map(sub_abbr)\n",
    "\n",
    "\n",
    "grouped = data.groupby(['Sub', 'Type'], observed=True).size().reset_index(name='Count')\n",
    "fig = px.bar(grouped, x='Sub', y='Count', color='Type', title='Object Subtype by Type') \n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [    0,         2,                4,                  6,                 8,              10,                  12,           3000]\n",
    "labels = ['Brilliant (<2 Mag)', 'Bright (<4 Mag)', 'Visible (<6 Mag)', 'Dim (<8 Mag)', 'Faint (<10 Mag)', 'Ghostly (<12 Mag)', 'Ultra Faint (12+ Mag)']\n",
    "data['VisualBin'] = pd.cut(data['Visual'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "Vz_enum = {abbr: idx for idx, abbr in enumerate(labels)}\n",
    "Vz_lookup = {idx: name for idx, name in enumerate(labels)}\n",
    "data['Vz'] = data['VisualBin'].map(Vz_enum).astype('Int64')\n",
    "data['VzName'] = data['Vz'].map(Vz_lookup)\n",
    "\n",
    "\n",
    "# Step 2: Group by SizeBin and Type\n",
    "grouped = data.groupby(['VisualBin', 'Type'], observed=True).size().reset_index(name='Count')\n",
    "\n",
    "# Step 3: Plot with color by Type\n",
    "fig = px.bar(grouped, x='VisualBin', y='Count', color='Type',\n",
    "             title='Object Apparent Mag Distribution by Type',\n",
    "             category_orders={'VisualBin': labels})  # ensures correct bin order\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [    0,      0.5,      1,      2,      5,       10,       30,        100,      3000]\n",
    "labels = ['Tiny (<0.5′)', 'Small (0.5–1′)', 'Compact (1–2′)', 'Moderate (2–5′)', 'Prominent (5–10′)', 'Wide (10–30′)', 'Extended (30–100′)', 'Expansive (100′+)']\n",
    "data['SizeBin'] = pd.cut(data['Size'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "Sz_enum = {abbr: idx for idx, abbr in enumerate(labels)}\n",
    "Sz_lookup = {idx: name for idx, name in enumerate(labels)}\n",
    "data['Sz'] = data['SizeBin'].map(Sz_enum)\n",
    "data['SzName'] = data['Sz'].map(Sz_lookup)\n",
    "\n",
    "\n",
    "# Step 2: Group by SizeBin and Type\n",
    "grouped = data.groupby(['SizeBin', 'Type'], observed=True).size().reset_index(name='Count')\n",
    "\n",
    "# Step 3: Plot with color by Type\n",
    "fig = px.bar(grouped, x='SizeBin', y='Count', color='Type',\n",
    "             title='Object Size Distribution by Type (Arcmin)',\n",
    "             category_orders={'SizeBin': labels})  # ensures correct bin order\n",
    "\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rt_lookup = {\n",
    "    5: 'Showcase (top 2%)',\n",
    "    4: 'Excellent (top 10%)',\n",
    "    3: 'Good (top 25%)',\n",
    "    2: 'Typical',\n",
    "    1: 'Challenging',\n",
    "    0: 'Not recommended'\n",
    "}\n",
    "data['Rt'] = data['Rating']\n",
    "data['RtName'] = data['Rt'].map(Rt_lookup)\n",
    "grouped = data.groupby(['Rating', 'Type'], observed=True).size().reset_index(name='Count')\n",
    "fig = px.bar(grouped, x='Rating', y='Count', color='Type', title='Object Rating by Type' ) \n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "catalogs = [\n",
    "    ('Messier', 'M'),\n",
    "    ('Caldwell', 'Caldwell '),\n",
    "    ('NGC', 'NGC '),\n",
    "    ('IC', 'IC '),\n",
    "    ('AbellG', 'Abell '),\n",
    "    ('AbellN', 'Abell '),\n",
    "    ('Arp', 'Arp '),\n",
    "    ('H400', 'H'),\n",
    "    ('UGC', 'UGC '),\n",
    "    ('PGC', 'PGC '),\n",
    "    ('Hickson', 'Hickson '),\n",
    "    ('Griff.', 'Griffiths '),\n",
    "    ('Kohou.', 'Kohoutek '),\n",
    "    ('Mink.', 'Minkowski '),\n",
    "    ('Barn.', 'Barnard '),\n",
    "    ('Sh2', 'Sh 2-'),\n",
    "    ('LBN', 'LBN '),\n",
    "    ('RCW', 'RCW '),\n",
    "    ('vdB', 'vdB '),\n",
    "    ('LDN', 'LDN '),\n",
    "    ('SNR', 'SNR '),\n",
    "    ('Gum', 'Gum '),\n",
    "    ('HT', 'HT '),\n",
    "    ('SD', 'SD '),\n",
    "    ('OB', 'OB '),\n",
    "    ('SP', 'SP '),\n",
    "    ('FG', 'FG '),\n",
    "    ('Name', ''),\n",
    "    ('Object', ''),\n",
    "]\n",
    "\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "\n",
    "def normalize_catalog_id(text):\n",
    "    # Remove leading zeros from catalog numbers (e.g. \"Abell 01\" → \"Abell 1\")\n",
    "    text = re.sub(r'(\\D+)\\s*0+(\\d+)', r'\\1 \\2', str(text))\n",
    "    # Remove trailing patterns like \" (3 of 5)\"\n",
    "    text = re.sub(r'\\s*\\(\\s*\\d+\\s*of\\s*\\d+\\s*\\)', '', text)\n",
    "    # Rename AbellG to Abell\"\n",
    "    text = re.sub(r'^AbellG\\b', 'Abell', text)\n",
    "    # Remove space in Messier eg M 23 to M23\n",
    "    text = re.sub(r'^M\\s*(\\d+)', r'M\\1', text)\n",
    "    # Rename Sh2 to Sh 2\n",
    "    text = re.sub(r'^Sh2', r'Sh 2', text)\n",
    "    # Single spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def get_catalog_ids(row):\n",
    "    primary = None\n",
    "    nickname = None\n",
    "    secondary = []\n",
    "    for col, prefix in catalogs:\n",
    "        val = row.get(col)\n",
    "        if pd.notnull(val):\n",
    "            formatted = f\"{prefix}{val}\"\n",
    "            normalized = normalize_catalog_id(formatted)\n",
    "            if not primary:\n",
    "                primary = normalized\n",
    "            else:\n",
    "                if col=='Name': \n",
    "                    nickname = normalized\n",
    "                elif col!='Object':\n",
    "                    secondary.append(normalized)\n",
    "                else:\n",
    "                    if not (normalized==nickname or normalized==primary):\n",
    "                        secondary.append(normalized)\n",
    "\n",
    "    unique_secondary = list(OrderedDict.fromkeys(secondary))\n",
    "    return pd.Series({ \n",
    "        'MainID': primary, \n",
    "        'OtherIDs': ', '.join(unique_secondary) if unique_secondary else None,\n",
    "        'Name': nickname,\n",
    "    })\n",
    "\n",
    "data[['MainID', 'OtherIDs', 'Name']] = data.apply(get_catalog_ids, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppliment data\n",
    "data.loc[data['OtherIDs'] == 'SMC', 'Name'] = 'Small Magellanic Cloud'\n",
    "data.loc[data['MainID'] == 'LMC', 'Name'] = 'Large Magellanic Cloud'\n",
    "data.loc[data['MainID'] == 'NGC 2327', 'Name'] = 'Seagull Nebula'\n",
    "data.loc[data['MainID'] == 'Dark Doodad', 'Name'] = 'Dark Doodad'\n",
    "data.loc[data['MainID'] == 'Dark Doodad', 'MainID'] = 'NGC 4372'\n",
    "data.loc[data['MainID'] == 'Caldwell 106', 'Name'] = '47 Tucanae'\n",
    "data.loc[data['MainID'] == 'vdB 21', 'Name'] = 'Maia Nebula'\n",
    "data.loc[data['MainID'] == 'vdB 22', 'Name'] = 'Merope Nebula'\n",
    "data.loc[data['MainID'] == 'vdB 23', 'Name'] = 'Pleiades Nebula'\n",
    "data.loc[data['MainID'] == 'Caldwell 50', 'Name'] = 'Satellite Cluster'\n",
    "data.loc[data['MainID'] == 'Caldwell 77', 'Name'] = 'Centaurus A'\n",
    "data.loc[data['MainID'] == 'NGC 7822', 'Name'] = 'Cosmic Question Mark'\n",
    "data.loc[data['MainID'] == 'Chamaeleon I', 'Name'] = 'Chamaeleon dark cloud'\n",
    "data.loc[data['OtherIDs'] == 'Chamaeleon II', 'Name'] = 'Haast Eagle Nebula, Possum Nebula'\n",
    "data.loc[data['OtherIDs'] == 'Chamaeleon II', 'MainID'] = 'Chamaeleon II'\n",
    "data.loc[data['OtherIDs'] == 'Chamaeleon II', 'OtherIDs'] = None\n",
    "data.loc[data['MainID'] == 'Chamaeleon III', 'Name'] = 'Chamaeleon dark cloud'\n",
    "data.loc[data['MainID'] == 'LBN 468', 'Name'] = 'Gyulbudaghian\\'s Nebula'\n",
    "data.loc[data['MainID'] == 'LDN 1251', 'Name'] = 'Rotten Fish Nebula'\n",
    "data.loc[data['MainID'] == 'Abell 262', 'Name'] = 'Galaxy Cluster in Andromeda'\n",
    "data.loc[data['MainID'] == 'Barnard 7', 'Name'] = 'Taurus Molecular Clouds'\n",
    "data.loc[data['MainID'] == 'NGC 7822', 'Name'] = 'Cosmic Question Mark'\n",
    "data.loc[data['MainID'] == 'NGC 7822', 'Name'] = 'Cosmic Question Mark'\n",
    "data.loc[data['MainID'] == 'NGC 7822', 'Name'] = 'Cosmic Question Mark'\n",
    "data.loc[data['MainID'] == 'NGC 7822', 'Name'] = 'Cosmic Question Mark'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['MainID', 'Name', 'VzName', 'C1Name', 'C2Name', 'CnName', 'SzName', 'RtName', 'Notes','OtherIDs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(C1_lookup)\n",
    "print(C2_lookup)\n",
    "print(Cn_lookup)\n",
    "print(Sz_lookup)\n",
    "print(Rt_lookup)\n",
    "print(Vz_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "selected_columns = ['MainID', 'Name', 'Notes','Class', 'OtherIDs', 'Rt', 'Sz', 'Vz', 'Cn', 'C1', 'C2', 'RA_deg', 'Dec_deg']\n",
    "filtered_df = data[\n",
    "    data['Rt'].isin([5, 4, 3]) &\n",
    "    ~data['Vz'].isin([6, 5]) &\n",
    "    data['Sz'].isin([7, 6, 5])\n",
    "][selected_columns]\n",
    "sorted_df = filtered_df.sort_values(by=['Rt', 'Sz'], ascending=[False, False])\n",
    "json_str = sorted_df.to_json(orient='records', indent=2)\n",
    "pathname = '../pilot/public/catalog_a_lg.json'\n",
    "print(pathname,len(sorted_df))\n",
    "with open(pathname, 'w') as f:\n",
    "    f.write(json_str)\n",
    "\n",
    "\n",
    "filtered_df = data[\n",
    "    data['Rt'].isin([5, 4, 3]) &\n",
    "    ~data['Vz'].isin([6, 5]) &\n",
    "    data['Sz'].isin([4, 3, 2, 1, 0])\n",
    "][selected_columns]\n",
    "sorted_df = filtered_df.sort_values(by=['Rt', 'Sz'], ascending=[False, False])\n",
    "json_str = sorted_df.to_json(orient='records', indent=2)\n",
    "pathname = '../pilot/public/catalog_a_md.json'\n",
    "print(pathname,len(sorted_df))\n",
    "with open(pathname, 'w') as f:\n",
    "    f.write(json_str)\n",
    "\n",
    "\n",
    "filtered_df = data[\n",
    "    data['Rt'].isin([5, 4, 3]) &\n",
    "    data['Vz'].isin([5])\n",
    "][selected_columns]\n",
    "sorted_df = filtered_df.sort_values(by=['Rt', 'Sz'], ascending=[False, False])\n",
    "json_str = sorted_df.to_json(orient='records', indent=2)\n",
    "pathname = '../pilot/public/catalog_a_sm.json'\n",
    "print(pathname,len(sorted_df))\n",
    "with open(pathname, 'w') as f:\n",
    "    f.write(json_str)\n",
    "\n",
    "filtered_df = data[\n",
    "    data['Rt'].isin([5, 4, 3]) &\n",
    "    data['Vz'].isin([6])\n",
    "][selected_columns]\n",
    "sorted_df = filtered_df.sort_values(by=['Rt', 'Sz'], ascending=[False, False])\n",
    "json_str = sorted_df.to_json(orient='records', indent=2)\n",
    "pathname = '../pilot/public/catalog_a_xs.json'\n",
    "print(pathname,len(sorted_df))\n",
    "with open(pathname, 'w') as f:\n",
    "    f.write(json_str)\n",
    "\n",
    "filtered_df = data[\n",
    "    data['Rt'].isin([2]) &\n",
    "    ~data['Vz'].isin([6, 5]) \n",
    "][selected_columns]\n",
    "sorted_df = filtered_df.sort_values(by=['Rt', 'Sz'], ascending=[False, False])\n",
    "json_str = sorted_df.to_json(orient='records', indent=2)\n",
    "pathname = '../pilot/public/catalog_b_md.json'\n",
    "print(pathname,len(sorted_df))\n",
    "with open(pathname, 'w') as f:\n",
    "    f.write(json_str)\n",
    "\n",
    "filtered_df = data[\n",
    "    data['Rt'].isin([2]) &\n",
    "    data['Vz'].isin([5])\n",
    "][selected_columns]\n",
    "sorted_df = filtered_df.sort_values(by=['Rt', 'Sz'], ascending=[False, False])\n",
    "json_str = sorted_df.to_json(orient='records', indent=2)\n",
    "pathname = '../pilot/public/catalog_b_sm.json'\n",
    "print(pathname,len(sorted_df))\n",
    "with open(pathname, 'w') as f:\n",
    "    f.write(json_str)\n",
    "\n",
    "\n",
    "filtered_df = data[\n",
    "    data['Rt'].isin([2]) &\n",
    "    data['Vz'].isin([6])\n",
    "][selected_columns]\n",
    "sorted_df = filtered_df.sort_values(by=['Rt', 'Sz'], ascending=[False, False])\n",
    "json_str = sorted_df.to_json(orient='records', indent=2)\n",
    "pathname = '../pilot/public/catalog_b_xs.json'\n",
    "print(pathname,len(sorted_df))\n",
    "with open(pathname, 'w') as f:\n",
    "    f.write(json_str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "a='░▁▂▃▄▅▆▇█▔░▁_▂▃▄▅▆▇▔'\n",
    "\n",
    "def categorize_alt(alt):\n",
    "    if alt < 0:\n",
    "        return 'Below Horizon'  # Not visible\n",
    "    elif alt < 12:\n",
    "        return 'Near Horizon'   # Rising or setting, poor visibility\n",
    "    elif alt < 30:\n",
    "        return 'Low Altitude'   # Often affected by atmospheric distortion\n",
    "    elif alt < 60:\n",
    "        return 'Mid Altitude'   # Good visibility, moderate elevation\n",
    "    elif alt < 82:\n",
    "        return 'High Altitude'  # Excellent visibility, optimal for imaging\n",
    "    else:\n",
    "        return 'Near Zenith'    # Peak elevation, polaris cannot reach\n",
    "\n",
    "\n",
    "def approx_altaz(ra_deg, dec_deg, observer_lat, observer_lon, time_utc):\n",
    "    # Convert to radians\n",
    "    ra = np.radians(ra_deg)\n",
    "    dec = np.radians(dec_deg)\n",
    "    lat = np.radians(observer_lat)\n",
    "\n",
    "    # Julian Date\n",
    "    jd = (time_utc - datetime(2000, 1, 1, tzinfo=timezone.utc)).total_seconds() / 86400.0 + 2451545.0\n",
    "\n",
    "    # Local Sidereal Time (LST) in degrees\n",
    "    lst_deg = (100.46 + 0.985647 * (jd - 2451545.0) + observer_lon + np.degrees(ra)) % 360\n",
    "    ha_rad = np.radians(lst_deg - ra_deg)  # Hour angle in radians\n",
    "\n",
    "    # Altitude\n",
    "    alt_rad = np.arcsin(np.sin(lat) * np.sin(dec) + np.cos(lat) * np.cos(dec) * np.cos(ha_rad))\n",
    "    alt_deg = np.degrees(alt_rad)\n",
    "\n",
    "    # Azimuth\n",
    "    cz = (np.sin(dec) - np.sin(lat) * np.sin(alt_rad)) / (np.cos(lat) * np.cos(alt_rad))\n",
    "    cz = np.clip(cz, -1, 1)  # Avoid domain errors\n",
    "    az_rad = np.arccos(cz)\n",
    "    az_deg = np.degrees(az_rad)\n",
    "    az_deg = np.where(np.sin(ha_rad) < 0, az_deg, 360 - az_deg)\n",
    "\n",
    "    return az_deg, alt_deg\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
